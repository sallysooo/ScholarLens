# í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ (ìµœì´ˆ 1íšŒ ì‹¤í–‰)
#!pip install -q transformers torch sentence-transformers PyMuPDF gradio keybert nltk

# ì „ì²´ êµ¬í˜„ ì½”ë“œ
import fitz
import re
import torch
import gradio as gr
from transformers import PegasusTokenizer, PegasusForConditionalGeneration
from keybert import KeyBERT
from nltk import sent_tokenize
import nltk
import subprocess

# NLTK ë¦¬ì†ŒìŠ¤ ë‹¤ìš´ë¡œë“œ
subprocess.run(["python", "-m", "nltk.downloader", "punkt"], check=True)

# GPU ë©”ëª¨ë¦¬ ê´€ë¦¬ ì´ˆê¸°í™”
def clear_gpu_cache():
    torch.cuda.empty_cache()
    if torch.cuda.is_available():
        torch.cuda.reset_peak_memory_stats()

# í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
def clean_text(text):
    # í—¤ë”/í‘¸í„° íŒ¨í„´ ì œê±°
    text = re.sub(r'Page\s+\d+\s+of\s+\d+', '', text)
    text = re.sub(r'\n\d+\n', '\n', text)
    text = re.sub(r'(\n\s*)+\n', '\n\n', text)
    
    # í‘œ/ê·¸ë¦¼ ì„¤ëª… ì œê±°
    text = re.sub(r'Table\s+\d+:.+?\n', '', text, flags=re.IGNORECASE)
    text = re.sub(r'Figure\s+\d+:.+?\n', '', text, flags=re.IGNORECASE)
    
    # íŠ¹ìˆ˜ ë¬¸ì ì •ê·œí™”
    text = re.sub(r'\(cid:\d+\)', '', text)
    text = re.sub(r'\xad', '', text)  # ì†Œí”„íŠ¸ í•˜ì´í”ˆ ì œê±°
    return text

# ì§€ëŠ¥í˜• í…ìŠ¤íŠ¸ ë¶„í• 
def smart_split(text, max_len=800):
    sentences = sent_tokenize(text)
    chunks = []
    current_chunk = []
    current_length = 0
    
    for sent in sentences:
        sent_len = len(sent.split())
        if current_length + sent_len > max_len and current_chunk:
            chunks.append(' '.join(current_chunk))
            current_chunk = []
            current_length = 0
        current_chunk.append(sent)
        current_length += sent_len
    
    if current_chunk:
        chunks.append(' '.join(current_chunk))
    return chunks

# ìš”ì•½ ëª¨ë¸ ì´ˆê¸°í™” (8ë¹„íŠ¸ ì–‘ìí™” ì ìš©)
model_name = 'google/pegasus-large'
tokenizer = PegasusTokenizer.from_pretrained(model_name)
model = PegasusForConditionalGeneration.from_pretrained(
    model_name, 
    device_map="auto", 
    load_in_8bit=True
)

# ê³„ì¸µì  ìš”ì•½ ì‹œìŠ¤í…œ
def summarize(text, max_length=200):
    try:
        inputs = tokenizer(
            text,
            truncation=True,
            max_length=1024,
            return_tensors="pt"
        ).to('cuda')
        
        summary_ids = model.generate(
            inputs["input_ids"],
            max_length=max_length,
            min_length=50,
            num_beams=4,
            early_stopping=True
        )
        return tokenizer.decode(summary_ids[0], skip_special_tokens=True)
    except Exception as e:
        clear_gpu_cache()
        raise RuntimeError(f"ìš”ì•½ ì‹¤íŒ¨: {str(e)}")

# ì„¹ì…˜ ë¶„í•  (ì‚¬ìš©ì ì •ì˜ í•„ìš”)
import re
from typing import List

def split_into_sections_advanced(text: str, max_len: int = 1500) -> List[str]:
    """ê³„ì¸µì  ì„¹ì…˜ ë¶„í•  ì‹œìŠ¤í…œ"""
    
    # 1ë‹¨ê³„: ì‹œê°ì  êµ¬ë¶„ì ê¸°ë°˜ ë¶„í•  (í—¤ë” ìŠ¤íƒ€ì¼ ê°ì§€)
    visual_patterns = [
        r'\n\s*[A-Z][A-Za-z\s]+\s*:\s*\n',  # "Introduction: " í˜•ì‹
        r'\n\s*[A-Z][A-Za-z\s]+[\d.]*\s*\n{2,}',  # 2ê°œ ì´ìƒ ì¤„ë°”ê¿ˆìœ¼ë¡œ êµ¬ë¶„
        r'\n\s*Â§\d+\.?\s+[A-Z]',  # "Â§1. Introduction" í˜•ì‹
        r'\n\s*\d+[\d.]*\s+[A-Z]'  # "1.1 Introduction" í˜•ì‹
    ]
    
    # 2ë‹¨ê³„: ì¼ë°˜ì ì¸ í•™ìˆ  ì„¹ì…˜ í‚¤ì›Œë“œ (í™•ì¥ ë²„ì „)
    section_keywords = [
        'abstract', 'introduction', 'related work', 'methodology',
        'experiment', 'results', 'discussion', 'conclusion',
        'acknowledg', 'reference', 'appendix', 'data availability',
        'conflict of interest', 'author contribution'
    ]
    
    # 3ë‹¨ê³„: í˜¼í•© íŒ¨í„´ ìƒì„±
    combined_patterns = []
    for kw in section_keywords:
        # ìˆ«ì+ì /ê³µë°± ì¡°í•© í—ˆìš© (ì˜ˆ: "1. ", "2 ")
        combined_patterns.append(rf'\n\s*\d*\.?\s*{kw}[^\n]*\s*\n')
        # ë³¼ë“œ/ì´íƒ¤ë¦­ ìŠ¤íƒ€ì¼ í—ˆìš© (ì˜ˆ: "**Introduction**")
        combined_patterns.append(rf'\n\s*[\*_]{{1,2}}{kw}[^\n]*[\*_]{{1,2}}\s*\n')
    
    # ëª¨ë“  íŒ¨í„´ í†µí•©
    all_patterns = visual_patterns + combined_patterns
    
    # ê²½ê³„ì  íƒì§€
    boundaries = []
    for pattern in all_patterns:
        for match in re.finditer(pattern, text, flags=re.IGNORECASE):
            start = match.start()
            # ê²½ê³„ì  ë³´ì • (ì„¹ì…˜ ì œëª© ì‹œì‘ ë¶€ë¶„ìœ¼ë¡œ ì¡°ì •)
            prev_newline = text.rfind('\n', 0, start)
            boundaries.append(prev_newline if prev_newline != -1 else start)
    
    # 4ë‹¨ê³„: í›„ì²˜ë¦¬
    boundaries = sorted(set(boundaries))
    
    # ì¸ì ‘ ê²½ê³„ì  ë³‘í•© (ìµœì†Œ 50ì ê°„ê²©)
    merged = []
    for b in boundaries:
        if not merged or b - merged[-1] > 50:
            merged.append(b)
    
    # 5ë‹¨ê³„: ì‹¤ì œ ë¶„í•  ìˆ˜í–‰
    sections = []
    for i in range(len(merged)):
        start = merged[i]
        end = merged[i+1] if i+1 < len(merged) else len(text)
        section = text[start:end].strip()
        if len(section) > 100:
            sections.append(section)
    
    # 6ë‹¨ê³„: ë¶„í•  ì‹¤íŒ¨ ì‹œ ê¸€ì ìˆ˜ ê¸°ë°˜ ë¶„í• 
    if not sections:
        return [text[i:i+max_len] for i in range(0, len(text), max_len)]
    
    return sections


# í•µì‹¬ ìš©ì–´ ì¶”ì¶œ
kw_model = KeyBERT()
def extract_key_terms(text, top_n=10):
    return [kw[0] for kw in kw_model.extract_keywords(
        text, 
        keyphrase_ngram_range=(1, 3), 
        stop_words='english', 
        top_n=top_n
    )]

# ì „ì²´ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
def full_pipeline(pdf_path):
    try:
        # 1. PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ
        doc = fitz.open(pdf_path)
        full_text = ""
        for page in doc:
            full_text += page.get_text()
        
        # 2. í…ìŠ¤íŠ¸ ì •ì œ
        cleaned = clean_text(full_text)
        
        # 3. ê³„ì¸µì  ìš”ì•½
        sections = split_into_sections(cleaned)
        section_summaries = [summarize(sect) for sect in sections[:5]]  # ìƒìœ„ 5ê°œ ì„¹ì…˜ë§Œ ì²˜ë¦¬
        combined = '\n'.join(section_summaries)
        final_summary = summarize(combined, max_length=300)
        
        # 4. í‚¤ì›Œë“œ ì¶”ì¶œ
        keywords = extract_key_terms(cleaned)
        
        return final_summary, keywords
    
    except Exception as e:
        clear_gpu_cache()
        return f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}", []

# Gradio ì¸í„°í˜ì´ìŠ¤
interface = gr.Interface(
    fn=full_pipeline,
    inputs=gr.File(label="ğŸ“„ PDF íŒŒì¼ ì—…ë¡œë“œ"),
    outputs=[
        gr.Textbox(label="ğŸ“ ìš”ì•½ ê²°ê³¼", lines=10),
        gr.HighlightedText(label="ğŸ”‘ í•µì‹¬ ìš©ì–´", 
                         combine_adjacent=True,
                         show_legend=True)
    ],
    title="ğŸ§  AI ë…¼ë¬¸ ìš”ì•½ ì‹œìŠ¤í…œ",
    description="í•™ìˆ  ë…¼ë¬¸ PDFë¥¼ ì—…ë¡œë“œí•˜ë©´ ìë™ìœ¼ë¡œ ìš”ì•½ê³¼ í•µì‹¬ ìš©ì–´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.",
    examples=[
        ["/content/sample.pdf"]  # ì˜ˆì‹œ íŒŒì¼ ê²½ë¡œ
    ]
)

# ì¸í„°í˜ì´ìŠ¤ ì‹¤í–‰
if __name__ == "__main__":
    interface.launch(debug=True, share=True)
